#!/usr/bin/env python

import math
import sys
import trainer.recurrent_units as recurrent

eos_id = 1
start_id = 0

generating = get_config_arg('generating', bool, False)
gen_trans_file = get_config_arg('gen_trans_file', str, None)
beam_size = get_config_arg('beam_size', int, 20)

print('generating : %d' % (generating))
if generating:
    print('gen_trans_file : %s' % (gen_trans_file))
#####################  Network Configuration ##############################################

src_lang_dict = "data/nested_layer_group/src.dict.wdwd"
src_dict_dim = len(open(src_lang_dict, 'r').readlines())

trg_lang_dict = "data/nested_layer_group/trg.dict.wdwd"
dict_for_gen = trg_lang_dict
trg_dict_dim = len(open(trg_lang_dict, 'r').readlines())

word_vec_dim = 256
latent_chain_dim = 256
clipping_thd = 50
lr_hid_col = 8e-2

max_frames = 250
res_per_sample = beam_size

default_decay_rate(2e-3)
default_num_batches_regularization(1)
default_initial_std(1 / math.sqrt(latent_chain_dim * 3.0))

model_type("recurrent_nn")
if generating:
    load_data_args = '{src_dict}'.format(src_dict=src_lang_dict)
    print('gen arg for pydata: %s' % (load_data_args))

    TestData(
        PyData(
            files=('gen.list'),
            load_data_module='gen_mt_data_nest_by_wrapper',
            load_data_object='processData',
            load_data_args=load_data_args))

    Settings(
        algorithm='sgd',
        batch_size=1,
        learning_rate=0, )
else:
    load_data_args = "{src_dict_path} {trg_dict_path}".format(
        src_dict_path=src_lang_dict, trg_dict_path=trg_lang_dict)
    TrainData(
        PyData(
            files=('train.list'),
            load_data_module='gen_mt_data_nest_by_wrapper',
            load_data_object='processData',
            load_data_args=load_data_args))
    Settings(
        algorithm='sgd',
        learning_method='adam',
        learning_rate=5e-4,
        do_average_in_cpu=True,
        learning_rate_decay_a=0,
        learning_rate_decay_b=0,
        ada_rou=0.95,
        ada_epsilon=1e-6,
        batch_size=4,
        average_window=0.5,
        max_average_window=2500 * 2,
        num_batches_per_send_parameter=1,
        num_batches_per_get_parameter=1, )

model_type("recurrent_nn")
DataLayer(
    name="src_words",
    size=src_dict_dim, )

if generating:
    Inputs(
        "src_words",
        "sent_id", )
    Outputs(
        "src_emb")  # dummuy but must be included in multi-thread generation
    Layer(
        name="sent_id",
        type="data",
        size=1, )
else:
    Inputs("src_words", "trg_words", "trg_next_words")
    Outputs("cost")

    DataLayer(
        name="trg_words",
        size=trg_dict_dim, )

    DataLayer(
        name="trg_next_words",
        size=trg_dict_dim, )

MixedLayer(
    name="src_emb",
    size=word_vec_dim,
    active_type="",
    bias=False,
    inputs=TableProjection(
        "src_words",
        initial_std=1 / math.sqrt(word_vec_dim),
        parameter_name="_src_emb"), )

RecurrentLayerGroupBegin(
    name="combine_multi_encoder",
    in_links=[Link("src_emb", has_subseq=True)],
    out_links=[
        Link("encoder_forward", has_subseq=True),
        Link("encoder_backward", has_subseq=True)
    ],
    seq_reversed=False, )
for direction in ["forward", "backward"]:
    recurrent.GatedRecurrentLayerGroup(
        name="encoder_" + direction,
        size=latent_chain_dim,
        para_prefix="encoder_" + direction,
        active_type="tanh",
        gate_active_type="sigmoid",
        inputs=[FullMatrixProjection("src_emb")],
        error_clipping_threshold=clipping_thd,
        seq_reversed=0 if (direction == 'forward') else 1, )
RecurrentLayerGroupEnd("combine_multi_encoder")

Layer(
    name="encoder_forward_last",
    type="seqlastins",
    bias=False,
    inputs=[Input("encoder_forward")])
Layer(
    name="encoder_backward_first",
    type="seqfirstins",
    bias=False,
    inputs=[Input("encoder_backward")])

Layer(
    name="encoder",
    type="concat",
    inputs=["encoder_forward_last", "encoder_backward_first"])

MixedLayer(
    name="encoder_proj",
    size=latent_chain_dim,
    active_type="tanh",
    bias=False,
    inputs=FullMatrixProjection("encoder"), )

#################################### decoder #######################################
if not generating:
    MixedLayer(
        name="trg_emb",
        size=word_vec_dim,
        active_type="tanh",
        bias=False,
        inputs=TableProjection(
            "trg_words",
            initial_std=1 / math.sqrt(word_vec_dim),
            parameter_name="_trg_emb"), )


RecurrentLayerGroupBegin(
    name = "decoder_layer_group",
    in_links = [] if generating else ["trg_emb"],
    out_links = ["predict_word"] if generating else ["output"],
    seq_reversed = False,
    generator = Generator(
        max_num_frames = max_frames,\
        beam_size = beam_size,\
        num_results_per_sample = beam_size) if generating else None,
)

if generating:
    predict_word_memory = Memory(
        name="predict_word",
        size=trg_dict_dim,
        boot_with_const_id=start_id, )
    MixedLayer(
        name="trg_emb",
        size=word_vec_dim,
        active_type="",
        bias=False,
        inputs=TableProjection(predict_word_memory, parameter_name="_trg_emb"),
    )

encoder_memory = Memory(
    name="encoder_read_memory",
    boot_layer="encoder_proj",
    boot_bias=False,
    size=latent_chain_dim,
    is_sequence=True, )
MixedLayer(
    name="encoder_read_memory",
    size=latent_chain_dim,
    active_type="",
    bias=False,
    inputs=IdentityProjection(encoder_memory), )

Layer(
    name="context_weights",
    type="mixed",
    size=1,
    active_type="sequence_softmax",
    bias=False,
    inputs=FullMatrixProjection(encoder_memory), )

Layer(
    name="context_vectors",
    type="scaling",
    inputs=["context_weights", encoder_memory], )

Layer(
    name="context",
    type="average",
    average_strategy="sum",
    inputs=["context_vectors"], )

recurrent.GatedRecurrentUnit(
    name="decoder",
    size=latent_chain_dim,
    active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[FullMatrixProjection("context"), FullMatrixProjection("trg_emb")],
    para_prefix="decoder",
    error_clipping_threshold=clipping_thd,
    out_memory=None)

Layer(
    name="output",
    type="mixed",
    size=trg_dict_dim,
    active_type="softmax",
    bias=Bias(parameter_name="_output.wbias"),
    inputs=FullMatrixProjection("decoder", parameter_name="_output.w"), )
if generating:
    Layer(
        name="predict_word",
        type="maxid",
        inputs=["output"], )
    Layer(
        name="eos_check",
        type="eos_id",
        eos_id=eos_id,
        inputs=["predict_word"], )
RecurrentLayerGroupEnd("decoder_layer_group")

if generating:
    Evaluator(
        name="target_printer",
        type="seq_text_printer",
        dict_file=dict_for_gen,
        result_file=gen_trans_file,
        inputs=["sent_id", "predict_word"], )
else:
    Layer(
        name="cost",
        type="multi-class-cross-entropy",
        inputs=["output", "trg_next_words"], )
    Evaluator(
        name="token_error_rate",
        type="classification_error",
        inputs=["output", "trg_next_words"])
