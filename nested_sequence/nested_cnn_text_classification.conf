#!usr/bin/env python
from paddle.trainer_config_helpers import *
import math
import os
import sys

default_decay_rate(2e-3)
default_num_batches_regularization(1)
default_initial_std(1e-3)
default_initial_strategy(0)

job = get_config_arg("job", int, 0)
model_type("recurrent_nn")

use_global_stats = True


def load_dict(dict_file):
    word_dict = {}
    with open(dict_file, "r") as fin:
        for idx, line in enumerate(fin):
            word_dict[line.strip().split("\t")[0]] = idx
    return word_dict


static = False
lr = 1
sub_seq_model = 0

word_dict_path = "word_dict.txt"
word_dict = load_dict(word_dict_path)
word_dict_size = len(word_dict)

label_dict_path = "label.dict"
label_dict = load_dict(label_dict_path)
label_dict_size = len(label_dict)

if job == 2:
    load_data_args = "%s %s" % (word_dict_path, label_dict_path)
    TestData(
        PyData(
            files=("test_raw.list"),
            load_data_module="txt_cla_for_eval",
            load_data_object="processData",
            load_data_args=load_data_args))
    batch_size = 4 * 64
    bn_type = "batch_norm"

elif job == 1:
    define_py_data_sources2(
        train_list=None,
        test_list="test_raw.list",
        module='txt_cla',
        obj='processData',
        args={
            "word_dict": word_dict,
            "label_dict": label_dict,
            "job_mode": job,
            "sub_seq_model": sub_seq_model
        })
    bn_type = "batch_norm"
    batch_size = 4 * 32
else:
    define_py_data_sources2(
        train_list="train_raw.list",
        test_list=None,
        module='txt_cla',
        obj='processData',
        args={
            "word_dict": word_dict,
            "label_dict": label_dict,
            "job_mode": 0,
            "sub_seq_model": sub_seq_model
        })
    batch_size = 4 * 40
    bn_type = "cudnn_batch_norm"

Settings(
    algorithm='sgd',
    batch_size=batch_size,
    learning_rate=0.1 / batch_size / 10,
    learning_method='momentum',
    learning_rate_decay_a=0.1,
    learning_rate_decay_b=56742 * 10,
    learning_rate_schedule="discexp", )


def bottom_layer(size,
                 input_name,
                 window_size,
                 name,
                 static=False,
                 lr=1,
                 dr=0.):
    Layer(
        name=name + 'context1',
        type="mixed",
        bias=False,
        inputs=ContextProjection(
            input_name,
            context_start=0,
            context_length=window_size,
            trainable_padding=False))

    Layer(
        name=name + 'conv0',
        type="mixed",
        drop_rate=dr,
        size=size,
        active_type="linear",
        bias=Bias(
            initial_std=1e-1,
            initial_mean=0,
            is_static=static,
            learning_rate=lr),
        inputs=[
            FullMatrixProjection(
                name + "context1",
                initial_std=2e-2,
                is_static=static,
                learning_rate=lr)
        ])

    Layer(
        name=name + 'batch_norm0',
        type='batch_norm',
        batch_norm_type=bn_type,
        active_type="relu",
        use_global_stats=use_global_stats,
        bias=Bias(
            initial_mean=0.1,
            initial_std=0,
            is_static=static,
            learning_rate=lr),
        inputs=Input(
            name + 'conv0',
            initial_mean=1.0,
            initial_std=0.0,
            is_static=static,
            learning_rate=lr,
            image=Image(channels=size, img_size=1)), )

    Layer(
        name=name + 'conv1',
        type="mixed",
        drop_rate=dr,
        size=size,
        active_type="linear",
        bias=Bias(
            initial_std=1e-1,
            initial_mean=0,
            is_static=static,
            learning_rate=lr),
        inputs=[
            FullMatrixProjection(
                name + "context1",
                is_static=static,
                learning_rate=lr,
                initial_std=2e-2, ),
        ], )

    Layer(
        name=name + 'batch_norm1',
        type='batch_norm',
        batch_norm_type=bn_type,
        active_type="relu",
        use_global_stats=use_global_stats,
        bias=Bias(
            initial_mean=0.0,
            initial_std=0.0,
            is_static=static,
            learning_rate=lr),
        inputs=Input(
            name + 'conv1',
            initial_mean=0,
            initial_std=0.1,
            is_static=static,
            learning_rate=lr,
            image=Image(channels=size, img_size=1)), )

    Layer(
        name=name + 'context2',
        type="mixed",
        bias=False,
        inputs=ContextProjection(
            name + 'batch_norm1',
            context_start=0,
            context_length=window_size,
            trainable_padding=False))

    Layer(
        name=name + 'conv2',
        type="mixed",
        drop_rate=dr,
        size=size,
        active_type="linear",
        bias=Bias(
            initial_std=1e-1,
            initial_mean=0,
            is_static=static,
            learning_rate=lr),
        inputs=[
            FullMatrixProjection(
                name + "context2",
                initial_std=2e-2,
                is_static=static,
                learning_rate=lr),
        ], )

    Layer(
        name=name + 'batch_norm2',
        type='batch_norm',
        batch_norm_type=bn_type,
        active_type="linear",
        use_global_stats=use_global_stats,
        bias=Bias(
            initial_mean=0.,
            initial_std=0.1,
            is_static=static,
            learning_rate=lr),
        inputs=Input(
            name + 'conv2',
            initial_mean=1.0,
            initial_std=0.0,
            is_static=static,
            learning_rate=lr,
            image=Image(channels=size, img_size=1)), )

    Layer(
        name=name + 'residual',
        type="mixed",
        size=size,
        active_type="relu",
        bias=Bias(
            initial_std=1e-1,
            initial_mean=0,
            is_static=static,
            learning_rate=lr),
        inputs=[
            IdentityProjection(name + "batch_norm0"),
            IdentityProjection(name + "batch_norm2")
        ], )


def residual_layer(size,
                   input_name,
                   window_size,
                   name,
                   static=False,
                   lr=1,
                   dr=0.):
    Layer(
        name=name + 'context1',
        type="mixed",
        bias=False,
        inputs=ContextProjection(
            input_name,
            context_start=0,
            context_length=window_size,
            trainable_padding=False))

    Layer(
        name=name + 'conv1',
        type="mixed",
        drop_rate=dr,
        size=size,
        active_type="linear",
        bias=Bias(
            initial_std=1e-1,
            initial_mean=0,
            is_static=static,
            learning_rate=lr),
        inputs=[
            FullMatrixProjection(
                name + "context1",
                initial_std=2e-2,
                is_static=static,
                learning_rate=lr),
        ], )

    Layer(
        name=name + 'batch_norm1',
        type='batch_norm',
        batch_norm_type=bn_type,
        active_type="relu",
        use_global_stats=use_global_stats,
        bias=Bias(
            initial_mean=0.0,
            initial_std=0.0,
            is_static=static,
            learning_rate=lr),
        inputs=Input(
            name + 'conv1',
            initial_mean=0,
            initial_std=0.1,
            is_static=static,
            learning_rate=lr,
            image=Image(channels=size, img_size=1)), )

    Layer(
        name=name + 'context2',
        type="mixed",
        bias=False,
        inputs=ContextProjection(
            name + 'batch_norm1',
            context_start=0,
            context_length=window_size,
            trainable_padding=False))

    Layer(
        name=name + 'conv2',
        type="mixed",
        drop_rate=dr,
        size=size,
        active_type="linear",
        bias=Bias(
            initial_std=0.1,
            initial_mean=0,
            is_static=static,
            learning_rate=lr),
        inputs=[
            FullMatrixProjection(
                name + "context2",
                initial_std=2e-2,
                is_static=static,
                learning_rate=lr),
        ], )

    Layer(
        name=name + 'batch_norm2',
        type='batch_norm',
        batch_norm_type=bn_type,
        use_global_stats=use_global_stats,
        active_type="linear",
        bias=Bias(
            initial_mean=0.0,
            initial_std=0.1,
            is_static=static,
            learning_rate=lr),
        inputs=Input(
            name + 'conv2',
            is_static=static,
            learning_rate=lr,
            initial_mean=1.0,
            initial_std=0.0,
            image=Image(channels=size, img_size=1)), )

    Layer(
        name=name + 'residual',
        type="mixed",
        size=size,
        active_type="relu",
        bias=Bias(
            initial_std=0.1,
            initial_mean=0,
            is_static=static,
            learning_rate=lr),
        inputs=[
            IdentityProjection(input_name),
            IdentityProjection(name + "batch_norm2")
        ], )


def residual_block(input_name,
                   output,
                   window_size,
                   conv_size,
                   num_ite,
                   static=False,
                   lr=1,
                   dr=0.):
    bottom_layer(conv_size, input_name, window_size, output + "_t1_", static,
                 lr, dr)
    for ite in range(1, num_ite):
        residual_layer(conv_size, output + "_t" + str(ite) + "_residual",
                       window_size, output + "_t" + str(ite + 1) + "_", static,
                       lr, dr)


def residual_network(input_layer):
    residual_block(
        input_name=input_layer,
        output="block_1",
        window_size=3,
        conv_size=128,
        num_ite=3,
        dr=0.)
    residual_block(
        input_name="block_1_t3_residual",
        output="block_2",
        window_size=3,
        conv_size=256,
        num_ite=3,
        dr=0.)
    return "block_2_t3_residual"


def nest_cnn_enc(input_seq, group_name="enc"):
    cnn_out_name = "block_2_t3_residual"
    RecurrentLayerGroupBegin(
        name=group_name,
        in_links=[Link(input_seq, has_subseq=True)],
        out_links=[Link(cnn_out_name, has_subseq=True)],
        seq_reversed=False)
    residual_network(input_seq)
    RecurrentLayerGroupEnd(group_name)
    return cnn_out_name


if job != 2:
    Inputs("words", "label")
    Outputs("cost")
else:
    Inputs("words", "label", "raw_txt")
    Outputs("label", "predict", "raw_txt")
    Layer(type="data", name="raw_txt", size=2048)

Layer(type="data", name="words", size=word_dict_size)
Layer(type="data", name="label", size=label_dict_size)
Layer(
    name="emb",
    type="mixed",
    size=128,
    bias=False,
    active_type="linear",
    inputs=TableProjection("words", parameter_name="_emb", initial_std=1e-3), )
enc_out = nest_cnn_enc("emb")

Layer(
    name="fea_avg",
    inputs=[Input(enc_out)],
    bias=False,
    drop_rate=0.35,
    active_type="linear",
    type="average",
    average_strategy="average",
    trans_type="non-seq", )

Layer(
    name="output",
    type="mixed",
    active_type="softmax",
    size=label_dict_size,
    bias=Bias(initial_std=1e-3),
    inputs=[FullMatrixProjection("fea_avg", initial_std=1e-2)])

if job != 2:
    Layer(
        name="cost",
        type="multi-class-cross-entropy",
        inputs=["output", "label"], )
else:
    Layer(inputs=[Input("output")], type="maxid", name="predict")

Evaluator(
    name="error_rate", type="classification_error", inputs=["output", "label"])
