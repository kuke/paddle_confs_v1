#!/usr/env/bin python
import math
import os
import sys

import paddle.trainer.recurrent_units as recurrent
from paddle.trainer_config_helpers import *

use_scheduled_sampling = 0
attention_with_decoder_idx = 0

generating = get_config_arg("generating", int, 0)
gen_trans_file = get_config_arg("gen_trans_file", str, "gen.txt")

word_dict_path = "data/dialog_word_count_20.txt"
dict_for_gen = word_dict_path
word_dict = {}
with open(word_dict_path, "r") as fdict:
    for idx, line in enumerate(fdict):
        word_dict[line.strip()] = idx
dict_dim = len(word_dict)

word_vec_dim = 256
latent_chain_dim = 512

clipping_thd = 50
encoder_depth = 2
decoder_depth = 2

# for beam search
max_frames = 100
beam_size = 5
res_per_sample = get_config_arg("beam_search_res_num", int, beam_size)

# for sequence generation
eos_id = word_dict["<e>"]
start_id = word_dict["<s>"]

lr_hid_col = 1e-1
default_decay_rate(1e-4)
default_num_batches_regularization(1)
default_initial_std(1 / math.sqrt(4 * latent_chain_dim))

model_type("recurrent_nn")
# batch_size = 4 * 40
batch_size = 4

if generating:
    define_py_data_sources2(
        train_list=None,
        test_list="data/test.list",
        module="gen_data",
        obj="processData1",
        args={
            "word_dict": word_dict,
            "job_mode": generating,
        })
    batch_size = 4 * 16
else:
    define_py_data_sources2(
        train_list="data/train.list",
        test_list=None,
        module="gen_data",
        obj="processData1",
        args={
            "word_dict": word_dict,
            "job_mode": generating,
            "scheduled_sampling": 1,
        })

Settings(
    algorithm="sgd",
    learning_method="adadelta",
    #learning_method = "adam",
    #learning_method = "rmsprop",
    learning_rate=1e-2,
    learning_rate_decay_a=0,
    learning_rate_decay_b=0,
    ada_rou=0.95,
    ada_epsilon=1e-6,
    batch_size=batch_size,
    average_window=0.5,
    do_average_in_cpu=True,
    max_average_window=2500,
    num_batches_per_send_parameter=1,
    num_batches_per_get_parameter=1, )

#==================================  network =================================


def BidirectionEncoderLstmmemory(name, input, size, depth, with_attention):
    global lr_hid_col
    for i in range(1, depth + 1):
        for direction in ["forward", "backward"]:
            Layer(
                name="{n}_{d}_input_recurrent{idx:02d}".format(
                    n=name, d=direction, idx=i),
                type="mixed",
                size=size * 4,
                bias=True,
                inputs=[
                    IdentityProjection(
                        "{n}_{d}_input_recurrent{idx:02d}".format(
                            n=name, d=direction, idx=i - 1),
                        learning_rate=lr_hid_col), FullMatrixProjection(
                            "{n}_{d}_lstm{idx:02d}".format(
                                n=name, d=direction, idx=i - 1),
                            initial_std=1e-3)
                ] if i > 1 else [FullMatrixProjection(input)])
            Layer(
                name="{n}_{d}_lstm{idx:02d}".format(
                    n=name, d=direction, idx=i),
                #device = 1,
                type="lstmemory",
                reversed=not i % 2 if direction == "forward" else i % 2,
                active_type="tanh",
                active_gate_type="sigmoid",
                active_state_type="tanh",
                bias=Bias(initial_std=0),
                inputs=Input(
                    "{n}_{d}_input_recurrent{idx:02d}".format(
                        n=name, d=direction, idx=i),
                    initial_std=0.01), )

    if not with_attention:
        Layer(
            name=name + "_forward_last",
            #device = 1,
            type="seqlastins",
            bias=False,
            inputs=[
                Input("{n}_forward_lstm{idx:02d}".format(n=name, idx=depth))
            ])
        Layer(
            name=name + "_backward_first",
            #device = 1,
            type="seqfirstins",
            bias=False,
            inputs=[
                Input("{n}_backward_lstm{idx:02d}".format(n=name, idx=depth))
            ])

    Layer(
        name=name,
        type="concat",
        #device = 1,
        inputs=[
            name + "_forward_lstm{idx:02d}".format(idx=encoder_depth),
            name + "_backward_lstm{idx:02d}".format(idx=encoder_depth)
        ] if with_attention else
        [name + "_forward_last", name + "_backward_first"])


#==========================================  network ==============================================
Layer(
    name="src_word",
    type="data",
    size=dict_dim, )

if generating:
    Inputs("sent_id", "src_word")

    Layer(name="sent_id", type="data", size=1)
else:
    Inputs("src_word", "trg_word", "trg_next_word", "true_token_flag")
    Outputs("cost")

    Layer(
        name="trg_word",
        type="data",
        size=dict_dim, )
    Layer(
        name="trg_next_word",
        type="data",
        size=dict_dim, )
    Layer(
        name="true_token_flag",
        type="data",
        size=2, )

Layer(
    name="src_emb",
    type="mixed",
    size=word_vec_dim,
    bias=False,
    inputs=TableProjection(
        "src_word", initial_std=0.01, parameter_name="_emb"), )

encoder_output = "encoder"

BidirectionEncoderLstmmemory(
    name=encoder_output,
    input="src_emb",
    size=latent_chain_dim,
    depth=encoder_depth,
    with_attention=True)  # output size is 2 * latent_chain_dim

MixedLayer(
    name="encoder_proj",
    size=latent_chain_dim,
    active_type="",
    bias=False,
    inputs=FullMatrixProjection(encoder_output))

#===========================  decoder ==========================

if not generating:
    Layer(
        name="trg_emb",
        type="mixed",
        size=word_vec_dim,
        bias=False,
        inputs=TableProjection(
            "trg_word", initial_std=0.01, parameter_name="_emb"), )

RecurrentLayerGroupBegin(
    "decoder_layer_group",
    in_links=[] if generating else ["trg_emb", "true_token_flag"],
    out_links=["predicted_word"] if generating else ["output"],
    seq_reversed=False,
    generator=Generator(
        max_num_frames=max_frames,
        beam_size=beam_size,
        num_results_per_sample=res_per_sample) if generating else None, )

predict_word_memory = Memory(
    name="predicted_word",
    size=dict_dim,
    boot_with_const_id=start_id, )
MixedLayer(
    name="predicted_emb",
    size=word_vec_dim,
    active_type="",
    bias=False,
    inputs=TableProjection(
        predict_word_memory,
        initial_std=1 / math.sqrt(word_vec_dim),
        parameter_name="_emb"), )

if not generating:
    Layer(
        name="trg_emb_multiplex",
        type="multiplex",
        size=word_vec_dim,
        inputs=["true_token_flag", "trg_emb", "predicted_emb"])

decoder_input = "predicted_emb" if generating else "trg_emb_multiplex"

encoder_output_memory = Memory(
    name="encoder_output_read_memory",
    size=latent_chain_dim * 2,
    boot_layer=encoder_output,
    boot_bias=False,
    is_sequence=True)
MixedLayer(
    name="encoder_output_read_memory",  # read-only memory
    size=latent_chain_dim * 2,
    active_type="",
    bias=False,
    inputs=IdentityProjection(encoder_output_memory), )

encoder_output_proj_memory = Memory(
    name="encoder_output_proj_read_memory",
    size=latent_chain_dim,
    boot_layer="encoder_proj",
    is_sequence=True)
MixedLayer(
    name="encoder_output_proj_read_memory",  # read-only memory
    size=latent_chain_dim,
    active_type="",
    bias=False,
    inputs=IdentityProjection(encoder_output_proj_memory), )

decoder_state_memory = []
for i in range(1, decoder_depth + 1):
    decoder_state_memory.append(
        Memory(
            name="decoder_unit{idx:02d}".format(idx=i),
            size=latent_chain_dim,
            is_sequence=False))

Layer(
    name="decoder_state_projected",
    type="mixed",
    size=latent_chain_dim,
    active_type="",
    bias=False,
    inputs=FullMatrixProjection(
        decoder_state_memory[attention_with_decoder_idx]), )

Layer(
    name="expand_decoder_state_projected",
    type="expand",
    inputs=["decoder_state_projected", encoder_output_memory], )

Layer(
    name="attention_vecs",
    type="mixed",
    size=latent_chain_dim,
    active_type="tanh",
    bias=False,
    inputs=[
        IdentityProjection("expand_decoder_state_projected"),
        IdentityProjection(encoder_output_proj_memory)
    ])

Layer(
    name="attention_weight",
    type="mixed",
    size=1,
    active_type="sequence_softmax",
    bias=False,
    inputs=FullMatrixProjection("attention_vecs"), )

Layer(
    name="context_vectors",
    type="scaling",
    inputs=["attention_weight", encoder_output_memory], )

Layer(  # reduce sequence
    name="context",
    type="average",
    average_strategy="sum",
    inputs=["context_vectors"], )

#==================================== decoder =======================================
for i in range(1, decoder_depth + 1):
    MixedLayer(
        name="decoder_input_recurrent{idx:02d}".format(idx=i),
        size=latent_chain_dim * 4,
        active_type="",
        bias=False,
        inputs=[
            FullMatrixProjection(
                "decoder_input_recurrent{idx:02d}".format(idx=i - 1),
                learning_rate=lr_hid_col),
            FullMatrixProjection("decoder_unit{idx:02d}".format(idx=i - 1))
        ] if i > 1 else
        [FullMatrixProjection("context"), FullMatrixProjection(decoder_input)])
    recurrent.LstmRecurrentUnit(
        name="decoder_unit{idx:02d}".format(idx=i),
        size=latent_chain_dim,
        active_type="tanh",
        state_active_type="tanh",
        gate_active_type="sigmoid",
        inputs=[
            IdentityProjection(
                "decoder_input_recurrent{idx:02d}".format(idx=i))
        ],
        para_prefix="decoder_unit{idx:02d}".format(idx=i),
        error_clipping_threshold=clipping_thd,
        out_memory=decoder_state_memory[i - 1])

Layer(
    name="output",
    type="mixed",
    size=dict_dim,
    active_type="softmax",
    bias=Bias(parameter_name="_output.wbias", initial_std=0),
    inputs=FullMatrixProjection(
        "decoder_unit{idx:02d}".format(idx=decoder_depth)))

Layer(
    name="predicted_word",
    type="maxid",
    inputs=["output"], )

if generating:
    Layer(
        name="eos_check",
        type="eos_id",
        eos_id=eos_id,
        inputs=["predicted_word"], )
RecurrentLayerGroupEnd("decoder_layer_group")

if generating:
    Evaluator(
        name="target_printer",
        type="seq_text_printer",
        dict_file=dict_for_gen,
        result_file=gen_trans_file,
        inputs=[
            "sent_id",
            "predicted_word",
        ], )
else:
    Layer(
        name="cost",
        type="multi-class-cross-entropy",
        inputs=["output", "trg_next_word"], )

    Evaluator(
        name="token_error_rate",
        type="classification_error",
        inputs=["output", "trg_next_word"])
