#!/usr/bin/env python
#coding:gbk

import math
import sys

source_language_dict_dim = 19554  #english
target_language_dict_dim = 21787  # ch_basic

latent_chain_dim = 256  #1024
word_vec_dim = latent_chain_dim * 2  #1024 #620

generating = get_config_arg('generating', bool, False)
with_cost = get_config_arg('with_cost', bool, True)
gen_trans_file = get_config_arg('gen_trans_file', str, None)
dict_file = get_config_arg('dict_for_gen', str, None)

print >> sys.stderr, 'generating = %d' % (generating)
print >> sys.stderr, 'gen_trans_file = %s' % (gen_trans_file)
print >> sys.stderr, 'dict_file = %s' % (dict_file)

default_initial_mean(0)
# for sequence generation
eos_id = 1
start_id = 0

model_type("recurrent_nn")

if generating:
    TestData(ProtoData(type="proto_sequence", files=('gen.list')))
    Settings(
        learning_rate=0,
        batch_size=1,
        algorithm='sgd', )

else:
    TrainData(ProtoData(type="proto_sequence", files=('train.list')))
    TestData(ProtoData(type="proto_sequence", files=('test.list')))

    Settings(
        learning_rate=1e-3,
        batch_size=22,
        algorithm='sgd',
        learning_method='adam',  #'rmsprop',
        num_batches_per_send_parameter=1,
        num_batches_per_get_parameter=1, )


##########################################################lstm unit
def LstmRecurrentUnit(name,
                      size,
                      active_type,
                      state_active_type,
                      gate_active_type,
                      inputs,
                      error_clipping_threshold=0,
                      state_memory=None):

    if state_memory is None:
        state_memory = Memory(name=name + "_" + "state", size=size)

    Layer(
        name=name + "_" + "input_recurrent",
        type="mixed",
        size=size * 4,  #(input_s, input_gate, forget_gate, output_gate)
        bias=False,
        error_clipping_threshold=error_clipping_threshold,
        inputs=inputs, )
    ExpressionLayer(
        name=name + "_" + "input_s",
        size=size,
        active_type=active_type,
        inputs=[
            IdentityOffsetProjection(name + "_" + "input_recurrent", offset=0)
        ], )
    ExpressionLayer(
        name=name + "_" + "input_gate",
        size=size,
        active_type=gate_active_type,
        inputs=[
            IdentityOffsetProjection(
                name + "_" + "input_recurrent", offset=size)
        ], )
    ExpressionLayer(
        name=name + "_" + "forget_gate",
        size=size,
        active_type=gate_active_type,
        inputs=[
            IdentityOffsetProjection(
                name + "_" + "input_recurrent", offset=size * 2)
        ], )
    ExpressionLayer(
        name=name + "_" + "output_gate",
        size=size,
        active_type=gate_active_type,
        inputs=[
            IdentityOffsetProjection(
                name + "_" + "input_recurrent", offset=size * 3)
        ], )
    ExpressionLayer(
        name=name + "_" + "state",
        inputs=[
            DotMulOperator([name + "_" + "input_s",
                            name + "_" + "input_gate"]),
            DotMulOperator([state_memory, name + "_" + "forget_gate"]),
        ], )
    ExpressionLayer(
        name=name,
        active_type=state_active_type,
        inputs=DotMulOperator(
            [name + "_" + "state", name + "_" + "output_gate"]), )


# like LstmRecurrentUnit, but it's a layer group.
# recurrent first dimension
# *inputs* second and more dimensions
# return concated all dimension input layer name
def LstmRecurrentLayerGroup(name,
                            size,
                            active_type,
                            state_active_type,
                            gate_active_type,
                            inputs,
                            parameter_name=None,
                            error_clipping_threshold=0,
                            boot_layer=None,
                            state_boot_layer=None,
                            seq_reversed=False):

    input_all_dimensions_layer_name = name + "_" + "input_all_dimensions"

    RecurrentLayerGroupBegin(
        name + "_layer_group",
        in_links=inputs,
        out_links=[
            name, input_all_dimensions_layer_name, name + "_" + "state"
        ],
        seq_reversed=seq_reversed)

    out_memory = Memory(name=name, size=size, boot_layer=boot_layer)
    state_memory = Memory(
        name=name + "_" + "state",
        size=size,
        boot_layer=state_boot_layer, )

    Layer(
        name=input_all_dimensions_layer_name,
        type="concat",
        active_type="",
        inputs=[Input(out_memory)] + inputs,  # first dimension
    )

    LstmRecurrentUnit(
        name=name,
        size=size,
        active_type=active_type,
        state_active_type=state_active_type,
        gate_active_type=gate_active_type,
        inputs=[
            FullMatrixProjection(
                input_all_dimensions_layer_name, parameter_name=parameter_name)
        ],
        error_clipping_threshold=error_clipping_threshold,
        state_memory=state_memory, )

    RecurrentLayerGroupEnd(name + "_layer_group")

    return input_all_dimensions_layer_name


def split_layer(name, size):
    ExpressionLayer(
        name=name + "_" + "first_half",
        size=size,
        inputs=[IdentityOffsetProjection(name, offset=0)], )
    ExpressionLayer(
        name=name + "_" + "last_half",
        size=size,
        inputs=[IdentityOffsetProjection(name, offset=size)], )


#################################### network #######################################
DataLayer(
    name="source_language_word",
    size=source_language_dict_dim, )

if generating:
    Inputs("sent_id", "source_language_word")
    Outputs("predict_word")

    Layer(
        name="sent_id",
        type="data",
        size=1,  #info for printer
    )

else:
    Inputs("source_language_word", "target_language_word",
           "target_language_next_word")
    Outputs("cost")

    DataLayer(
        name="target_language_word",
        size=target_language_dict_dim, )

    DataLayer(
        name="target_language_next_word",
        size=target_language_dict_dim, )

    MixedLayer(
        name="target_embedding",
        size=word_vec_dim,
        active_type="",
        bias=False,
        inputs=TableProjection(
            "target_language_word",
            initial_std=1 / math.sqrt(word_vec_dim),
            parameter_name="_target_language_embedding"), )

#################################### encoder #######################################
MixedLayer(
    name="source_embedding",
    size=word_vec_dim,
    active_type="",
    bias=False,
    inputs=TableProjection(
        "source_language_word",
        initial_std=1 / math.sqrt(word_vec_dim),
        parameter_name="_source_language_embedding"), )
split_layer("source_embedding", latent_chain_dim)

#################################### decoder #######################################
RecurrentLayerGroupBegin(
    "decoding_layer_group",
    in_links=[] if generating else ["target_embedding"],
    out_links=["predict_word"] if generating else ["decoder_out"],
    generator=Generator(
        max_num_frames=50, beam_size=5, num_results_per_sample=5)
    if generating else None, )

if generating:
    predict_word_memory = Memory(
        name="predict_word",
        size=target_language_dict_dim,
        boot_with_const_id=start_id, )
    MixedLayer(
        name="target_embedding",
        size=word_vec_dim,
        active_type="",
        bias=False,
        inputs=TableProjection(
            predict_word_memory,
            initial_std=1 / math.sqrt(word_vec_dim),
            parameter_name="_target_language_embedding"), )

memory_decoder_lstm1_out = Memory(
    name="decoder_lstm1",
    size=latent_chain_dim,
    boot_layer="source_embedding_first_half",
    is_sequence=True)
memory_decoder_lstm2_out = Memory(
    name="decoder_lstm2",
    size=latent_chain_dim,
    boot_layer="source_embedding_first_half",  # maybe zero boot
    is_sequence=True)

############################################# grid lstm start

split_layer("target_embedding", latent_chain_dim)

all_dimensions_input_lstm1 = LstmRecurrentLayerGroup(
    name="anotation_lstm1",
    size=latent_chain_dim,
    active_type="tanh",
    state_active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[memory_decoder_lstm1_out],
    parameter_name="anotation_lstm1.w",
    error_clipping_threshold=10.0,
    boot_layer="target_embedding_first_half",
    state_boot_layer="target_embedding_last_half", )

LstmRecurrentUnit(
    name="decoder_lstm1",
    size=latent_chain_dim,
    active_type="tanh",
    state_active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[
        FullMatrixProjection(
            all_dimensions_input_lstm1, parameter_name="decoder_lstm1.w")
    ],
    error_clipping_threshold=10.0,
    state_memory=Memory(
        name="decoder_lstm1" + "_" + "state",
        size=latent_chain_dim,
        is_sequence=True,
        boot_layer="source_embedding_last_half", ), )

Layer(
    name="grid_layer1_out",  #dimension 3 output
    type="mixed",
    size=latent_chain_dim,
    bias=False,
    inputs=[
        FullMatrixProjection("anotation_lstm1"),
        FullMatrixProjection("decoder_lstm1")
    ], )

Layer(  # for memory init
    name="anotation_lstm1_last",
    type="seqlastins",
    inputs=[Input("anotation_lstm1")], )
Layer(  # for memory init
    name="anotation_lstm1_state_last",
    type="seqlastins",
    inputs=[Input("anotation_lstm1_state")], )
all_dimensions_input_lstm2 = LstmRecurrentLayerGroup(
    name="anotation_lstm2",
    size=latent_chain_dim,
    active_type="tanh",
    state_active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[
        memory_decoder_lstm2_out,  # dim 2
        "grid_layer1_out",  # dim 3
    ],
    parameter_name="anotation_lstm2.w",
    error_clipping_threshold=10.0,
    seq_reversed=True,
    boot_layer="anotation_lstm1_last",
    state_boot_layer="anotation_lstm1_state_last", )

LstmRecurrentUnit(
    name="decoder_lstm2",
    size=latent_chain_dim,
    active_type="tanh",
    state_active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[
        FullMatrixProjection(
            all_dimensions_input_lstm2, parameter_name="decoder_lstm2.w")
    ],
    error_clipping_threshold=10.0,
    state_memory=Memory(
        name="decoder_lstm2" + "_" + "state",
        size=latent_chain_dim,
        boot_layer="source_embedding_last_half",  # maybe be zero
        is_sequence=True), )

Layer(
    name="decoder_layer2_out",
    type="concat",
    active_type="",
    inputs=[Input("anotation_lstm2"), Input("anotation_lstm2_state")], )

Layer(  # reduce sequence
    name="decoder_out",
    type="seqfirstins",
    inputs=[Input("decoder_layer2_out")], )

################################################### grid lstm end

if generating:
    Layer(
        name="output",
        type="mixed",
        size=target_language_dict_dim,
        active_type="softmax",
        bias=Bias(parameter_name="_output.wbias", initial_std=0),
        inputs=FullMatrixProjection(
            "decoder_out",
            initial_std=1 / math.sqrt(target_language_dict_dim),
            learning_rate=1,
            parameter_name="_output.w"), )
    Layer(
        name="predict_word",
        type="maxid",
        inputs=["output"], )
    Layer(
        name="eos_check",
        type="eos_id",
        eos_id=eos_id,
        inputs=["predict_word"], )

RecurrentLayerGroupEnd("decoding_layer_group")

if generating:
    Evaluator(
        name="target_printer",
        type="seq_text_printer",
        dict_file=dict_file,
        result_file=gen_trans_file,
        inputs=[
            "sent_id",
            "predict_word",
        ], )
else:
    Layer(
        name="output",
        type="mixed",
        size=target_language_dict_dim,
        active_type="softmax",
        bias=Bias(initial_std=0),
        inputs=FullMatrixProjection("decoder_out", parameter_name="_output.w"),
    )

    Layer(
        name="cost",
        type="multi-class-cross-entropy",
        inputs=["output", "target_language_next_word"], )

    Evaluator(
        name="token_error_rate",
        type="classification_error",
        inputs=["output", "target_language_next_word"])
