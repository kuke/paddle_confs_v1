#!/usr/bin/env python
#coding:gbk

import math
import sys
import trainer.recurrent_units as recurrent

source_language_dict_dim = 30000 + 3
target_language_dict_dim = 30000 + 3

word_vec_dim = 620
latent_chain_dim = 1000
default_initial_mean(0)

generating = get_config_arg('generating', bool, False)

gen_trans_file = './gen_trans/fr2en_without_attention_50_gen_en.txt'
dict_file = './gen_trans/en_dict'
eos_id = 1
start_id = 0

if generating:
    TestData(ProtoData(type="proto_sequence", files=('test.list')))
    Settings(
        learning_rate=0,
        batch_size=1,
        algorithm='sgd', )
else:
    TrainData(ProtoData(type="proto_sequence", files=('train.list')))
    TestData(ProtoData(type="proto_sequence", files=('test.list')))

    Settings(
        learning_rate=1e-3,
        batch_size=55,
        algorithm='sgd',
        learning_method='rmsprop',
        num_batches_per_send_parameter=1,
        num_batches_per_get_parameter=1, )

#################################### network #######################################
model_type("recurrent_nn")
DataLayer(
    name="source_language_word",
    size=source_language_dict_dim, )

if generating:
    Inputs("sent_id", "source_language_word")
    Outputs("predict_word")
    Layer(
        name="sent_id",
        type="data",
        size=1,  #info for printer
    )
else:
    Inputs("source_language_word", "target_language_word",
           "target_language_next_word")
    Outputs("cost")

    DataLayer(
        name="target_language_word",
        size=target_language_dict_dim, )

    DataLayer(
        name="target_language_next_word",
        size=target_language_dict_dim, )

MixedLayer(
    name="source_embedding",
    size=word_vec_dim,
    active_type="",
    bias=False,
    inputs=TableProjection(
        "source_language_word",
        initial_std=1 / math.sqrt(word_vec_dim),
        parameter_name="_source_language_embedding"), )

recurrent.GatedRecurrentLayerGroup(
    name="encoder_forward",
    size=latent_chain_dim,
    para_prefix="_forward",
    active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[
        FullMatrixProjection(
            "source_embedding", initial_std=1 / math.sqrt(latent_chain_dim))
    ],
    error_clipping_threshold=10.0,
    seq_reversed=False)

recurrent.GatedRecurrentLayerGroup(
    name="encoder_backward",
    size=latent_chain_dim,
    para_prefix="_backward",
    active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[
        FullMatrixProjection(
            "source_embedding", initial_std=1 / math.sqrt(latent_chain_dim))
    ],
    error_clipping_threshold=10.0,
    seq_reversed=True)

Layer(
    name="forward_last",
    type="seqlastins",
    active_type="",
    bias=False,
    inputs=[
        Input("encoder_forward"),
    ])

Layer(
    name="backward_last",
    type="seqlastins",
    active_type="",
    bias=False,
    inputs=[
        Input("encoder_backward"),
    ])

Layer(
    inputs=[Input("forward_last"), Input("backward_last")],
    name="encoder_last",
    active_type="",
    type="concat")

MixedLayer(
    name="encoder_last_projected",
    active_type="tanh",
    size=latent_chain_dim,
    bias=False,
    inputs=FullMatrixProjection("encoder_last"), )

#################################### decoder #######################################
decoder_name = "decoder"
if not generating:
    MixedLayer(
        name="target_embedding",
        size=word_vec_dim,
        active_type="tanh",
        bias=False,
        inputs=TableProjection(
            "target_language_word",
            initial_std=1 / math.sqrt(word_vec_dim),
            parameter_name="_target_language_embedding"), )

RecurrentLayerGroupBegin(
    decoder_name + "_layer_group",
    in_links=[] if generating else ["target_embedding"],
    out_links=["predict_word"] if generating else [decoder_name],
    seq_reversed=False,
    generator=Generator(
        max_num_frames=50, beam_size=5, num_results_per_sample=5)
    if generating else None, )

if generating:
    predict_word_memory = Memory(
        name="predict_word",
        size=target_language_dict_dim,
        boot_with_const_id=start_id, )
    MixedLayer(
        name="target_embedding",
        size=word_vec_dim,
        active_type="",
        bias=False,
        inputs=TableProjection(
            predict_word_memory,
            initial_std=1 / math.sqrt(word_vec_dim),
            parameter_name="_target_language_embedding"), )

encoder_last_memory = Memory(
    name="encoder_last_read_memory",
    boot_layer="encoder_last_projected",
    boot_bias=False,
    size=latent_chain_dim,
    is_sequence=False)
MixedLayer(
    name="encoder_last_read_memory",  # read-only memory
    size=latent_chain_dim,
    active_type="",
    bias=False,
    inputs=IdentityProjection(encoder_last_memory), )

decoder_state_memory = Memory(
    name=decoder_name,
    boot_layer="encoder_last_projected",
    boot_bias=False,
    size=latent_chain_dim,
    is_sequence=False)

recurrent.GatedRecurrentUnit(
    name=decoder_name,
    size=latent_chain_dim,
    active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[
        "encoder_last_read_memory", FullMatrixProjection("target_embedding")
    ],
    para_prefix="_decoder",
    error_clipping_threshold=10.0,
    out_memory=decoder_state_memory, )

if generating:
    Layer(
        name="output",
        type="mixed",
        size=target_language_dict_dim,
        active_type="softmax",
        bias=Bias(parameter_name="_output.wbias", initial_std=0),
        inputs=FullMatrixProjection(
            decoder_name,
            initial_std=1 / math.sqrt(target_language_dict_dim),
            learning_rate=1,
            parameter_name="_output.w"), )
    Layer(
        name="predict_word",
        type="maxid",
        inputs=["output"], )
    Layer(
        name="eos_check",
        type="eos_id",
        eos_id=eos_id,
        inputs=["predict_word"], )
RecurrentLayerGroupEnd(decoder_name + "_layer_group")

if generating:
    Evaluator(
        name="target_printer",
        type="seq_text_printer",
        dict_file=dict_file,
        result_file=gen_trans_file,
        inputs=[
            "sent_id",
            "predict_word",
        ], )
else:
    Layer(
        name="output",
        type="mixed",
        size=target_language_dict_dim,
        active_type="softmax",
        bias=Bias(initial_std=0),
        inputs=FullMatrixProjection(decoder_name, parameter_name="_output.w"),
    )

    Layer(
        name="cost",
        type="multi-class-cross-entropy",
        inputs=["output", "target_language_next_word"], )

    Evaluator(
        name="token_error_rate",
        type="classification_error",
        inputs=["output", "target_language_next_word"])
