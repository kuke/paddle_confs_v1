#!/usr/bin/env python

import math
import os
import sys

batch_size = 1000

word_dim = 256
hidden_dim = 512

default_initial_std(1 / math.sqrt(hidden_dim))
lr = 0.1

######################## Algorithm Configuration ########################
default_decay_rate(1e-4)
default_num_batches_regularization(1)
default_gradient_clipping_threshold(25)
Settings(
    algorithm='sgd',
    learning_rate=0.1,
    learning_method='adadelta',
    ada_epsilon=1e-6,
    ada_rou=0.95,
    batch_size=batch_size,
    average_window=0.5,
    max_average_window=2500,
    learning_rate_decay_a=0,
    learning_rate_decay_b=0, )

Inputs("firWord", "secWord", "thirdWord", "forthWord", "fifthWord", "nxtWord")
Outputs("cost")
vocab_path = './dict/dic.8w'
voc_dim = len(open(vocab_path).read().strip().split('\n'))
load_data_args = '{dic}'.format(dic=vocab_path)
TrainData(
    PyData(
        files='train.list',
        load_data_module='ngram',
        load_data_object='processData',
        load_data_args=load_data_args, ))


def dataLayer(layerName, layerSize):
    Layer(
        name=layerName,
        type="data",
        size=layerSize, )


dataLayer("firWord", voc_dim)
dataLayer("secWord", voc_dim)
dataLayer("thirdWord", voc_dim)
dataLayer("forthWord", voc_dim)
dataLayer("fifthWord", voc_dim)
dataLayer("nxtWord", voc_dim)


def firLayer(in_name, out_name):
    Layer(
        name=out_name,
        type="mixed",
        size=word_dim,
        bias=False,
        inputs=TableProjection(
            in_name,
            initial_std=0.001,
            initial_strategy=1,
            learning_rate=lr,
            decay_rate=0,
            #          decay_rate_l1 = l1,
            num_batches_regularization=100,
            parameter_name="emb"),
        device=-1, )


firLayer("firWord", "fir")
firLayer("secWord", "sec")
firLayer("thirdWord", "third")
firLayer("forthWord", "forth")
firLayer("fifthWord", "fifth")

Layer(
    name="embedding",
    type="concat",
    inputs=["fir", "sec", "third", "forth", "fifth"], )

Layer(
    name="hidden1",
    type="fc",
    active_type="sigmoid",
    size=hidden_dim,
    bias=Bias(learning_rate=2 * lr),
    inputs=Input(
        "embedding",
        learning_rate=lr,
        initial_std=1. / math.sqrt(word_dim * 8)),
    drop_rate=0.5, )

Layer(
    name="output",
    type="fc",
    active_type="softmax",
    size=voc_dim,
    bias=Bias(learning_rate=2 * lr),
    inputs=Input("hidden1", learning_rate=lr), )
Layer(
    name="cost",
    type="multi-class-cross-entropy",
    inputs=["output", "nxtWord"], )

Layer(
    name="error_layer",
    type="classification_error",
    inputs=["output", "nxtWord"])

Evaluator(
    name="error_classfication",
    type="sum",
    inputs="error_layer", )
