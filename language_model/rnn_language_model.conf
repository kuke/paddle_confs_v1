#!/usr/bin/env python

import json
import math
import os
import sys

Inputs("word", "next_word")
Outputs("cost")

TrainData(ProtoData(
    files=('train.list'), ))

batch_size = 10
default_decay_rate(1e-4 * batch_size)

word_dim = 23763

word_embedding_dim = 128
hidden_dim = 400
hidden_dim2 = 512

Layer(
    name="word",
    type="data",
    size=word_dim, )

Layer(
    name="next_word",
    type="data",
    size=word_dim, )

Layer(
    name="word_embedding",
    type="mixed",
    size=word_embedding_dim,
    bias=False,
    inputs=TableProjection(
        "word",
        parameter_name="_wordvecs.w",
        learning_rate=1,
        decay_rate=5e-4 * batch_size,
        initial_mean=0.0,
        initial_std=0.01), )
Layer(
    name="rnn_layer",
    type="recurrent",
    active_type="relu",
    bias=Bias(initial_std=0),
    inputs=Input("word_embedding", initial_std=0, learning_rate=1), )

Layer(
    name="output",
    type="mixed",
    size=word_dim,
    active_type="softmax",
    bias=Bias(initial_std=0),
    inputs=TransposedFullMatrixProjection(
        "rnn_layer", parameter_name="_output.w"), )

Layer(
    name="cost",
    type="multi-class-cross-entropy",
    inputs=["output", "next_word"], )

Settings(
    algorithm='sgd',
    learning_method='adagrad',
    ada_epsilon=1.0,
    batch_size=batch_size,
    learning_rate=0.1, )
