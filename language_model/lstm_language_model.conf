#!/usr/bin/env python
#coding:gbk

import json
import math
import os
import sys

Inputs("word", "next_word")
Outputs("cost")

TrainData(ProtoData('train.list'))

batch_size = 400
default_decay_rate(1e-4 * batch_size)

word_dim = 23763

word_embedding_dim = 128
hidden_dim = 400
hidden_dim2 = 512

Layer(
    name="word",
    type="data",
    size=word_dim, )

Layer(
    name="next_word",
    type="data",
    size=word_dim, )

Layer(
    name="word_embedding",
    type="mixed",
    size=word_embedding_dim,
    bias=False,
    inputs=TableProjection(
        "word",
        parameter_name="wordvecs",
        learning_rate=1,
        decay_rate=5e-4 * batch_size,
        initial_mean=0.0,
        initial_std=0.01), )

Layer(
    name="hidden1",
    type="mixed",
    size=hidden_dim,
    active_type="stanh",
    bias=True,
    inputs=[
        FullMatrixProjection("word_embedding"),
    ])

Layer(
    name="rnn1",
    type="lstmemory",
    active_type="tanh",  # relu
    active_state_type="",
    active_gate_type="sigmoid",
    bias=Bias(initial_std=0),
    inputs=Input("hidden1", initial_std=0, learning_rate=1), )

Layer(
    name="hidden2",
    type="mixed",
    size=hidden_dim2,
    active_type="stanh",
    inputs=[
        FullMatrixProjection("hidden1"),
        FullMatrixProjection("rnn1", initial_std=0, learning_rate=1),
    ], )

Layer(
    name="hidden3",
    type="mixed",
    size=word_embedding_dim,
    active_type="stanh",
    inputs=FullMatrixProjection(
        "hidden2", initial_std=math.sqrt(1. / hidden_dim2)), )

Layer(
    name="output",
    type="mixed",
    size=word_dim,
    active_type="softmax",
    bias=Bias(initial_std=0),
    inputs=TransposedFullMatrixProjection(
        "hidden3", parameter_name="wordvecs"), )

Layer(
    name="cost",
    type="multi-class-cross-entropy",
    inputs=["output", "next_word"], )

Settings(
    algorithm='sgd',
    learning_method='adagrad',
    ada_epsilon=1.0,
    batch_size=batch_size,
    learning_rate=0.1, )
