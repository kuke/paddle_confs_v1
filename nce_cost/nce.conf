#!/usr/bin/env python

################################### Data Configuration ###################################
TrainData(
    PyData(
        files="train.list",
        load_data_module="pyDataProvider",
        load_data_object="WordDataProvider",
        load_data_args="train.meta.list", ))

################################### Algorithm Configuration ###################################
bsize = 1200
lr = 0.1
algorithm('sgd')
batch_size(bsize)
learning_method('adagrad')
ada_epsilon(1.0)
learning_rate(lr)

num_batches_per_send_parameter(100)
num_batches_per_get_parameter(1)

################################### Network Configuration ###################################
word_dim = 200001
Layer(type="data", name="input", size=word_dim)
Layer(type="data", name="label", size=1)
Layer(
    inputs=[Input("input", parameter_name="_p_layer_projection.w")],
    bias=False,
    type="fc",
    name="embedding_layer",
    size=512)
Inputs("input", "label")
Outputs("cost")

################################### for negative sampling ######################################
import cPickle
pkl_file = open('./conf/all_pow_freq_list.pkl', 'rb')
neg_sampling_dist = cPickle.load(pkl_file)
pkl_file.close()

if get_config_arg('with_cost', bool, True):
    Layer(
        name="cost",
        type="nce",
        num_classes=word_dim,
        num_neg_samples=25,
        active_type="sigmoid",
        neg_sampling_dist=neg_sampling_dist,
        bias=Bias(parameter_name="_output.bias"),
        bias=False,
        inputs=[
            Input("embedding_layer", parameter_name="_p_embedding_layer.w"),
            "label",
        ], )
else:
    Layer(
        name="output",
        type="fc",
        size=word_dim,
        active_type="softmax",
        bias=Bias(parameter_name="_output.bias"),
        bias=False,
        inputs=[
            Input("embedding_layer", parameter_name="_p_embedding_layer.w"),
        ], )
    Layer(
        name="cost",
        type="multi-class-cross-entropy",
        inputs=["output", "label"], )
