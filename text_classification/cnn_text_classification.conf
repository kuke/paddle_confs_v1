#!usr/bin/env python
import math

Settings(
    algorithm='sgd',
    learning_method='adagrad',
    ada_epsilon=0.9,
    batch_size=110,
    average_window=0.5,
    max_average_window=2500,
    learning_rate=2e-2,
    learning_rate_decay_a=0,
    learning_rate_decay_b=0, )

TrainData(ProtoData(type="proto_sequence", files=('train.list')))

Inputs("word", "label")
Outputs("cost")

default_decay_rate(8e-4)

word_emb_dim = 64
hidden_dim = 256
lr_hid_col = 1e-1

context_length = 5
context_dim = context_length * word_emb_dim

default_initial_std(1 / math.sqrt(hidden_dim) / 3.0)

Layer(
    name="word",
    type="data",
    size=21915, )

Layer(
    name="word_embedding",
    type="mixed",
    size=word_emb_dim,
    bias=False,
    inputs=TableProjection(
        "word", initial_std=0, parameter_name="_emb", learning_rate=1e-4), )

Layer(
    name="word_context",
    type="mixed",
    bias=False,
    inputs=ContextProjection(
        "word_embedding",
        context_start=-(context_length - 1) / 2,
        context_length=context_length,
        trainable_padding=False))

Layer(
    name="word_conv",
    type="mixed",
    size=hidden_dim,
    active_type="relu",
    inputs=[
        FullMatrixProjection("word_context", learning_rate=lr_hid_col),
    ], )

Layer(
    name="max_pooling",
    type="max",
    bias=True,
    inputs=Input("word_conv", learning_rate=lr_hid_col), )

Layer(
    inputs=[Input("max_pooling", initial_std=0.01, parameter_name="hidden.w")],
    name="hidden",
    bias=Bias(parameter_name="hidden.bias"),
    active_type="tanh",
    type="fc",
    size=64)

Layer(
    name="softmax_out",
    type="mixed",
    active_type="softmax",
    size=2,
    bias=True,
    inputs=[
        FullMatrixProjection("hidden", learning_rate=lr_hid_col),
    ])

Layer(
    name="cost",
    type="multi-class-cross-entropy",
    inputs=[Input("softmax_out"), "label"], )

Evaluator(
    name="mis_classification",
    type="classification_error",
    inputs=["softmax_out", "label"])

Evaluator(
    name="spam_pre_rec",
    type="precision_recall",
    positive_label=1,
    inputs=["softmax_out", "label"])
