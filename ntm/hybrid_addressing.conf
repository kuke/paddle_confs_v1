#!/usr/bin/env python

import os
import sys
import math
import trainer.recurrent_units as recurrent

hybrid_addressing = 1

generating = get_config_arg('generating', bool, False)
gen_trans_file = get_config_arg('gen_trans_file', str, None)
beam_size = get_config_arg('beam_size', int, 1)

src_lang_dict = 'test_data/dict_for_gen/en.30k.dict'
source_language_dict_dim = len(open(src_lang_dict, 'r').readlines())
src_col = 0

trg_lang_dict = 'test_data/dict_for_gen/fr.30k.dict'
dict_for_gen = trg_lang_dict
target_language_dict_dim = len(open(trg_lang_dict, 'r').readlines())
trg_col = 1

generating = get_config_arg('generating', bool, False)
print('generating : %d' % (generating))
if generating:
    print('gen_trans_file : %s' % (gen_trans_file))
    print('dict_for_gen : %s' % (trg_lang_dict))

word_vec_dim = 512
latent_chain_dim = 512

encoder_depth = 1
decoder_depth = 1

# for beam search
max_frames = 250
start_id = 0
eos_id = 1

clipping_thd = 50
# default parameters
default_initial_strategy(0)  # 0 for normal, 1 for uniform
default_initial_smart(True)

default_decay_rate(8e-4)
default_num_batches_regularization(1)
default_gradient_clipping_threshold(50)

model_type('recurrent_nn')
if generating:
    load_data_args = '{job_mode} {hy_addr} {src_dict} {src_col}'.format(
        job_mode=1,
        hy_addr=hybrid_addressing,
        src_dict=src_lang_dict,
        src_col=src_col)
    print('gen arg for pydata: %s' % (load_data_args))

    TestData(
        PyData(
            files=('gen.list'),
            load_data_module='gen_data_nmt',
            load_data_object='processData',
            load_data_args=load_data_args))

    Settings(
        algorithm='sgd',
        batch_size=1,
        learning_rate=0, )
else:
    load_data_args = ('{job_mode} {hy_addr} {src_dict} '
                      '{src_col} {trg_dict} {trg_col}').format(
                          job_mode=0,
                          hy_addr=hybrid_addressing,
                          src_dict=src_lang_dict,
                          trg_dict=trg_lang_dict,
                          src_col=src_col,
                          trg_col=trg_col)

    TrainData(
        PyData(
            files=('train.list'),
            load_data_module='gen_data_nmt',
            load_data_object='processData',
            load_data_args=load_data_args))
    TestData(
        PyData(
            files=('test.list'),
            load_data_module='gen_data_nmt',
            load_data_object='processData',
            load_data_args=load_data_args))

    Settings(
        algorithm='sgd',
        learning_method='adam',
        learning_rate=5e-4,
        do_average_in_cpu=True,
        learning_rate_decay_a=0,
        learning_rate_decay_b=0,
        ada_rou=0.95,
        ada_epsilon=1e-6,
        batch_size=4 * 11,
        average_window=0.5,
        max_average_window=2500 * 2,
        num_batches_per_send_parameter=1,
        num_batches_per_get_parameter=1, )


#################################### network #######################################
def GruEncoder(name, input, size, depth, clipping_thd):
    for i in range(depth):
        for direction in ['forward', 'backward']:
            recurrent.GatedRecurrentLayerGroup(
                name='{n}_{d}_gru{idx:02d}'.format(n=name, d=direction, idx=i),
                size=size,
                para_prefix='{n}_{d}_gru{idx:02d}'.format(
                    n=name, d=direction, idx=i),
                active_type='tanh',
                gate_active_type='sigmoid',
                inputs=[
                    FullMatrixProjection(
                        input, initial_std=1 / math.sqrt(size))
                ] if not i else [
                    FullMatrixProjection('{n}_{d}_gru{idx:02d}'.format(
                        n=name, d=direction, idx=i - 1))
                ],
                error_clipping_threshold=clipping_thd,
                seq_reversed=(i % 2)
                if not direction == 'forward' else not (i % 2))

    Layer(
        inputs=[
            '{n}_{d}_gru{idx:02d}'.format(n=name, d=direction, idx=depth - 1)
            for direction in ['forward', 'backward']
        ],
        name=name,
        active_type='',
        type='concat')


DataLayer(
    name='source_language_word',
    size=source_language_dict_dim, )

if generating:
    Inputs('sent_id', 'source_language_word')
    if hybrid_addressing: Inputs('init_attention_weights')
    Outputs('source_embedding')

    DataLayer(
        name='sent_id',
        size=1,  #info for printer
    )

else:
    Inputs('source_language_word', 'target_language_word',
           'target_language_next_word')
    if hybrid_addressing: Inputs('init_attention_weights')
    Outputs('cost')

    DataLayer(
        name='target_language_word',
        size=target_language_dict_dim, )

    DataLayer(
        name='target_language_next_word',
        size=target_language_dict_dim, )

    MixedLayer(
        name='target_embedding',
        size=word_vec_dim,
        active_type='',
        bias=False,
        inputs=TableProjection(
            'target_language_word',
            initial_std=1 / math.sqrt(word_vec_dim),
            parameter_name='_target_language_embedding'), )

DataLayer(name='init_attention_weights', size=1)

#################################### encoder #######################################
MixedLayer(
    name='source_embedding',
    size=word_vec_dim,
    active_type='',
    bias=False,
    inputs=TableProjection(
        'source_language_word',
        initial_std=1 / math.sqrt(word_vec_dim),
        parameter_name='_source_language_embedding'), )

encoder_name = 'encoder'
GruEncoder(
    name=encoder_name,
    input='source_embedding',
    size=latent_chain_dim,
    depth=encoder_depth,
    clipping_thd=clipping_thd, )

#################################### decoder #######################################
MixedLayer(
    name='encoder_projected',
    active_type='',
    size=latent_chain_dim,
    bias=False,
    inputs=FullMatrixProjection(encoder_name), )

RecurrentLayerGroupBegin(
    name = 'decoding_layer_group',
    in_links = [] if generating else ['target_embedding'],
    out_links = ['predict_word'] if generating else ['output'],
    generator = Generator(\
        max_num_frames = max_frames,\
        beam_size = beam_size,
        num_results_per_sample = beam_size) if generating else None,
)

if generating:
    predict_word_memory = Memory(
        name='predict_word',
        size=target_language_dict_dim,
        boot_with_const_id=start_id, )
    MixedLayer(
        name='predicted_embedding',
        size=word_vec_dim,
        active_type='',
        bias=False,
        inputs=[
            TableProjection(
                predict_word_memory,
                initial_std=1 / math.sqrt(word_vec_dim),
                parameter_name='_target_language_embedding')
        ], )

decoder_name = 'decoder'
decoder_input = 'predicted_embedding' if generating else 'target_embedding'

decoder_state_memory = []
for i in range(decoder_depth):
    decoder_state_memory.append(
        Memory(
            name='{n}_gru{idx:02d}'.format(n=decoder_name, idx=i),
            size=latent_chain_dim,
            is_sequence=False,
            boot_layer=None))

encoder_out_memory = Memory(
    name='encoder_out_projected',
    size=latent_chain_dim,
    boot_layer='encoder_projected',
    is_sequence=True)
Layer(
    name='decoder_state_projected',
    type='mixed',
    size=latent_chain_dim,
    active_type='',
    bias=False,
    inputs=FullMatrixProjection(decoder_state_memory[0]), )

Layer(
    name='expand_decoder_state_projected',
    type='expand',
    inputs=['decoder_state_projected', encoder_out_memory], )

Layer(
    name='attention_vecs',
    type='mixed',
    size=latent_chain_dim,
    active_type='tanh',
    bias=False,
    inputs=[
        IdentityProjection('expand_decoder_state_projected'),
        IdentityProjection(encoder_out_memory)
    ])

Layer(
    name='attention_weights',
    type='mixed',
    size=1,
    active_type='sequence_softmax',
    bias=False,
    inputs=FullMatrixProjection('attention_vecs'), )

if hybrid_addressing:
    attention_weight_memory = Memory(
        name='attention_weights',
        size=1,
        is_sequence=True,
        boot_layer='init_attention_weights', )
    MixedLayer(
        name='addressing_gate',
        active_type='sigmoid',
        size=1,
        bias=False,
        inputs=[FullMatrixProjection(decoder_input)], )
    Layer(
        name = 'expand_addressing_gate',
        type = 'expand',
        inputs = ['addressing_gate',\
                  encoder_out_memory],
    )
    Layer(
        name='weight_interpolation',
        type='interpolation',
        inputs=[
            'expand_addressing_gate', 'attention_weights',
            attention_weight_memory
        ], )
    Layer(
        name='shifting_weights',
        type='mixed',
        size=3,
        active_type='softmax',
        bias=Bias(initial_std=0),
        inputs=FullMatrixProjection(decoder_input), )
    Layer(
        name='convolutional_shift',
        type='conv_shift',
        inputs=['weight_interpolation', 'shifting_weights'], )
    Layer(
        name='context_vectors',
        type='scaling',
        inputs=['convolutional_shift', encoder_out_memory], )
else:
    Layer(
        name='context_vectors',
        type='scaling',
        inputs=['attention_weights', encoder_out_memory], )

Layer(  # reduce sequence
    name='context',
    type='average',
    average_strategy='sum',
    inputs=['context_vectors'], )

Layer(
    name='encoder_out_projected',  # read-only memory
    type='mixed',
    size=latent_chain_dim,
    active_type='',
    bias=False,
    inputs=IdentityProjection(encoder_out_memory), )

for i in range(decoder_depth):
    recurrent.GatedRecurrentUnit(
        name='{n}_gru{idx:02d}'.format(n=decoder_name, idx=i),
        size=latent_chain_dim,
        active_type='tanh',
        gate_active_type='sigmoid',
        inputs=[
            FullMatrixProjection('context'),
            FullMatrixProjection(decoder_input),
        ] if not i else [
            FullMatrixProjection(
                '{n}_gru{idx:02d}'.format(n=decoder_name, idx=i - 1))
        ],
        error_clipping_threshold=clipping_thd,
        out_memory=decoder_state_memory[i], )

Layer(
    name='output',
    type='mixed',
    size=target_language_dict_dim,
    active_type='softmax',
    bias=Bias(parameter_name='_output.wbias', initial_std=0),
    inputs=FullMatrixProjection(
        '{n}_gru{idx:02d}'.format(n=decoder_name, idx=decoder_depth - 1),
        initial_std=1 / math.sqrt(target_language_dict_dim),
        parameter_name='_output.w'), )
if generating:
    Layer(
        name='predict_word',
        type='maxid',
        inputs=['output'], )
    Layer(
        name='eos_check',
        type='eos_id',
        eos_id=eos_id,
        inputs=['predict_word'], )

RecurrentLayerGroupEnd('decoding_layer_group')

if generating:
    Evaluator(
        name='target_printer',
        type='seq_text_printer',
        dict_file=trg_lang_dict,
        result_file=gen_trans_file,
        inputs=[
            'sent_id',
            'predict_word',
        ], )

else:
    Layer(
        name='cost',
        type='multi-class-cross-entropy',
        inputs=['output', 'target_language_next_word'], )

    Evaluator(
        name='token_error_rate',
        type='classification_error',
        inputs=['output', 'target_language_next_word'])
