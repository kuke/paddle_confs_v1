#!/usr/bin/env python
# coding: utf-8

from paddle.trainer_config_helpers import *
import math

Settings(
    algorithm='sgd',
    learning_rate=0.0002,
    batch_size=4000,
    learning_rate_decay_a=0,
    learning_rate_decay_b=0,
    average_window=0,
    max_average_window=0, )

word_emb_dim = 256
hidden_dim = 256
context_length = 3
default_decay_rate(1e-4)
default_initial_std(0.01)
dict_path = 'idict.sort.100w.1'
dict_len_basic = len(open(dict_path).read().strip().split('\n')) + 1
TrainData(
    PyData(
        files='train.list',
        load_data_module='dataprovider_fc_bow',
        load_data_object='processData',
        load_data_args=dict_path, ))
TestData(
    PyData(
        files='test.list',
        load_data_module='dataprovider_fc_bow',
        load_data_object='processData_test',
        load_data_args=dict_path, ))

Inputs("query", "title_left", "title_right", "label")
Outputs("cost")


def mk_bow_encode_layers(doc, dict_len, seg_type):
    Layer(
        name=doc,
        type="data",
        size=dict_len, )
    Layer(
        name="word_embedding_" + seg_type + "_" + doc,
        type="mixed",
        active_type='',
        size=word_emb_dim,
        bias=False,
        inputs=TableProjection(
            doc,
            parameter_name="_emb_" + seg_type,
            decay_rate=1e-4,
            initial_std=0.02,
            learning_rate=1), )
    Layer(
        inputs=[
            Input(
                "word_embedding_" + seg_type + "_" + doc,
                parameter_name="emb.trans.w",
                initial_std=0.03)
        ],
        bias=Bias(parameter_name="emb.trans.bias", initial_std=0.01),
        name="emb_trans" + doc,
        active_type="relu",
        type="fc",
        size=hidden_dim, )
    Layer(
        inputs=[Input("emb_trans" + doc)],
        name="encode_" + seg_type + "_" + doc,
        bias=Bias(parameter_name="_avg.bias_" + seg_type, initial_std=0.001),
        active_type="tanh",
        type="average",
        average_strategy="sum",
        trans_type="non-seq")


for doc in ["query", "title_left", "title_right"]:
    mk_bow_encode_layers(doc, dict_len_basic, 'basic')

Layer(
    inputs=[
        Input(
            "encode_basic_query",
            parameter_name="_hidden_query.w",
            initial_std=0.03)
    ],
    bias=Bias(parameter_name="_hidden_query.bias", initial_std=0.0005),
    name="hidden_query",
    active_type="tanh",
    type="fc",
    size=hidden_dim)

Layer(
    inputs=[
        Input(
            "encode_basic_title_left",
            parameter_name="_hidden_title.w",
            initial_std=0.03)
    ],
    bias=Bias(parameter_name="_hidden_title.bias", initial_std=0.0005),
    name="hidden_title_left",
    active_type="tanh",
    type="fc",
    size=hidden_dim)

Layer(
    inputs=[
        Input(
            "encode_basic_title_right",
            parameter_name="_hidden_title.w",
            initial_std=0.03)
    ],
    bias=Bias(parameter_name="_hidden_title.bias", initial_std=0.0005),
    name="hidden_title_right",
    active_type="tanh",
    type="fc",
    size=hidden_dim)
CosSimLayer(
    name='output_left',
    inputs=["hidden_query", "hidden_title_left"], )
CosSimLayer(
    name='output_right',
    inputs=["hidden_query", "hidden_title_right"], )

Layer(
    name="label",
    type="data",
    size=2, )
Layer(
    name="sigmoid_out_l",
    type="mixed",
    active_type="sigmoid",
    size=1,
    bias=False,
    inputs=[IdentityProjection("output_left")])
Evaluator(
    inputs=["sigmoid_out_l", "label"], name="left_auc", type="last-column-auc")
Layer(
    inputs=["output_left", "output_right", "label"],
    name="cost",
    type="rank-cost")
Outputs("cost")
