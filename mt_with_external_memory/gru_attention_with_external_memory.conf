#!/usr/bin/env python

import math
import sys
import trainer.recurrent_units as recurrent

src_lang_dict = "test_data/dicts/fr.30k.dict"
src_word_count = len(open(src_lang_dict, "r").readlines())

trg_lang_dict = "test_data/dicts/en.30k.dict"
trg_word_count = len(open(trg_lang_dict, "r").readlines())
dict_file = trg_lang_dict

generating = get_config_arg("generating", int, 0)
gen_trans_file = get_config_arg("gen_trans_file", str, None)
beam_size = get_config_arg("beam_size", int, 3)
batch_size = get_config_arg("batch_size", int, 1)

max_gen_frame_num = 250

word_vec_dim = 512
latent_chain_dim = 1024

with_cost = 1

# default parameters
default_initial_mean(0)
default_initial_strategy(0)  # 0 for normal, 1 for uniform
default_initial_smart(True)

default_decay_rate(1e-3)
default_num_batches_regularization(1)
default_gradient_clipping_threshold(25)

# for sequence generation
eos_id = 1
start_id = 0

src_col = 1
trg_col = 0


def _(n, idx):
    return "%s_%02d" % (n, idx)


def BidirectionalGruEncoder(name,
                            input_layer,
                            hidden_size,
                            encoder_depth,
                            init_state_for_decoder=True):
    for i in range(encoder_depth):
        recurrent.GatedRecurrentLayerGroup(
            name=_("annotation_forward", i),
            size=hidden_size,
            active_type="tanh",
            gate_active_type="sigmoid",
            inputs=[FullMatrixProjection(_("annotation_forward", i - 1))]
            if i > 0 else [FullMatrixProjection(input_layer)],
            seq_reversed=False)

        recurrent.GatedRecurrentLayerGroup(
            name=_("annotation_backward", i),
            size=hidden_size,
            active_type="tanh",
            gate_active_type="sigmoid",
            inputs=[FullMatrixProjection(_("annotation_backward", i - 1))]
            if i > 0 else [FullMatrixProjection(input_layer)],
            seq_reversed=True)

    if init_state_for_decoder:
        Layer(
            name=name + "_annotation_backward_last",
            type="seqfirstins",
            active_type="linear",
            bias=False,
            inputs=[
                Input(_("annotation_backward", encoder_depth - 1)),
            ])
        MixedLayer(
            name=name + "_annotation_last_projected",
            active_type="tanh",
            size=hidden_size,
            bias=False,
            inputs=FullMatrixProjection(name + "_annotation_backward_last"), )

    Layer(
        inputs=[
            Input(_("annotation_" + prefix, encoder_depth - 1))
            for prefix in ["forward", "backward"]
        ],
        name=name,
        active_type="",
        type="concat")
    if init_state_for_decoder: return name + "_annotation_last_projected"
    else: return None


def ExternalMemoryBankInit(name, encoder_out, perturbation):
    ###########################   state to boot memory bank #######################
    Layer(
        name="encoder_average",  # size = latent_chain_dim * 2
        type="average",
        active_type="tanh",
        average_strategy="average",
        inputs=[encoder_out], )

    Layer(
        name="encoder_average_expand",  # size = latent_chain_dim * 2, length = slot number of memory bank
        type="expand",
        inputs=["encoder_average", perturbation], )

    Layer(
        name="add_perturbation",
        type="addto",
        bias=False,
        active_type="",
        inputs=["encoder_average_expand", perturbation], )

    Layer(  # output of this layer is to initialize the memory bank
        name=name,
        type="mixed",
        size=latent_chain_dim,
        active_type="tanh",
        bias=False,
        inputs=FullMatrixProjection("add_perturbation"), )


# should including current input be benefical ??
def ReadExternalMemory(read_result_vec, read_head_weight, size, read_head_boot,
                       prev_decoder_state, external_memory):
    Layer(
        name="prev_decoder_state_expand",
        type="expand",
        inputs=[prev_decoder_state, external_memory], )
    read_weight_memory = Memory(
        name="read_head_weight",
        size=size,
        boot_layer=read_head_boot,
        is_sequence=True)

    # information read from external memory are determined by (1) previous
    # hidden state of decoder and (2) previous state of the external memory
    Layer(
        name="read_weight_current",
        type="mixed",
        size=size,
        active_type="softmax",
        bias=False,
        inputs=[
            FullMatrixProjection(external_memory),
            FullMatrixProjection("prev_decoder_state_expand")
        ])

    # Is recurrent connection be benefical ???
    # there is a PROBLEM : weights exposed by the "gate" are the SAME for each
    # memory slot, this may be not reasonable.
    Layer(
        name="read_weight_gate",
        type="mixed",
        size=size,
        active_type="sigmoid",
        bias=False,
        inputs=[
            FullMatrixProjection("prev_decoder_state_expand"),
        ], )

    # Interpolation of current and previous read weight.
    # This adds memory to the read weight.
    # The interpolotaion weights are calculated merely from previous decoder hidden state
    ExpressionLayer(
        name=read_head_weight,
        inputs=[
            DotMulOperator([read_weight_memory, "read_weight_gate"]),
            DotMulOperator(
                ["read_weight_current", "read_weight_gate"], scale=-1.0),
        ], )

    Layer(
        name="read_vecs",
        type="mixed",
        bias=False,
        size=size,
        inputs=DotMulOperator([read_head_weight, external_memory]), )

    Layer(
        name=read_result_vec,
        type="average",
        average_strategy="sum",
        inputs=["read_vecs"], )


# share weight with read head
def WriteExternalMemory(size, cur_decoder_state, prev_memory_state,
                        read_head_weight):
    Layer(
        name="cur_decoder_state_expand",
        type="expand",
        inputs=[cur_decoder_state, prev_memory_state], )

    # Is recurrent connection beneficial ?
    Layer(
        name="erase_candidate",
        type="mixed",
        size=size,
        active_type="sigmoid",
        bias=False,
        inputs=[FullMatrixProjection("cur_decoder_state_expand")])
    Layer(
        name="add_candidate",
        type="mixed",
        size=size,
        active_type="sigmoid",
        bias=False,
        inputs=[FullMatrixProjection("cur_decoder_state_expand")])

    Layer(
        name="erase_vector",
        type="mixed",
        size=latent_chain_dim,
        inputs=[
            DotMulOperator([read_head_weight, "erase_candidate"], scale=-1.0),
        ], )
    Layer(
        name="earsed_memory",
        type="mixed",
        size=latent_chain_dim,
        inputs=[
            DotMulOperator([prev_memory_state, "erase_vector"]),
        ], )

    Layer(
        name="add_vector",
        type="mixed",
        size=latent_chain_dim,
        inputs=[DotMulOperator([read_head_weight, "add_candidate"])], )

    Layer(
        name="external_memory",
        type="mixed",
        inputs=[
            IdentityProjection("earsed_memory"),
            IdentityProjection("add_vector")
        ], )


model_type("recurrent_nn")
if generating:
    load_data_args = "{job_mode} {src_dict} {src_col}".format(
        job_mode=1, src_dict=src_lang_dict, src_col=src_col)
    print("gen arg for pydata: %s" % (load_data_args))

    TestData(
        PyData(
            files=("gen.list"),
            load_data_module="gen_mt_data_with_external_memory",
            load_data_object="processData",
            load_data_args=load_data_args))

    Settings(algorithm="sgd", batch_size=batch_size, learning_rate=0)
else:
    load_data_args = ("{job_mode} {src_dict_path} {src_col} "
                      "{trg_dict_path} {trg_col}").format(
                          job_mode=generating,
                          src_dict_path=src_lang_dict,
                          src_col=src_col,
                          trg_dict_path=trg_lang_dict,
                          trg_col=trg_col)

    TrainData(
        PyData(
            files=("train.list"),
            load_data_module="gen_mt_data_with_external_memory",
            load_data_object="processData",
            load_data_args=load_data_args))

    TestData(
        PyData(
            files=("test.list"),
            load_data_module="gen_mt_data_with_external_memory",
            load_data_object="processData",
            load_data_args=load_data_args))

    Settings(
        algorithm="sgd",
        learning_method="adadelta",
        learning_rate=1e-1,
        do_average_in_cpu=True,
        learning_rate_decay_a=0,
        learning_rate_decay_b=0,
        ada_rou=0.95,
        ada_epsilon=1e-6,
        batch_size=11 * 8,
        average_window=0.5,
        max_average_window=2500 * 2,
        num_batches_per_send_parameter=1,
        num_batches_per_get_parameter=1, )

#################################### network #######################################
DataLayer(
    name="source_language_word",
    size=src_word_count, )

if generating:
    Inputs("sent_id", "source_language_word", "random_perturbation",
           "zero_read_weight")
    Outputs("predicted_word")

    Layer(name="sent_id", type="data", size=1)

else:
    Inputs("source_language_word", "target_language_word",
           "target_language_next_word", "random_perturbation",
           "zero_read_weight")
    Outputs("cost")

    DataLayer(
        name="target_language_word",
        size=trg_word_count, )

    DataLayer(
        name="target_language_next_word",
        size=trg_word_count, )

    MixedLayer(
        name="target_embedding",
        size=word_vec_dim,
        active_type="",
        bias=False,
        inputs=TableProjection(
            "target_language_word",
            initial_std=1 / math.sqrt(word_vec_dim),
            parameter_name="_target_language_embedding"), )

DataLayer(
    name="random_perturbation",
    size=latent_chain_dim * 2, )
DataLayer(
    name="zero_read_weight",
    size=latent_chain_dim, )

MixedLayer(
    name="source_embedding",
    size=word_vec_dim,
    active_type="",
    bias=False,
    inputs=TableProjection(
        "source_language_word",
        initial_std=1 / math.sqrt(word_vec_dim),
        parameter_name="_source_language_embedding"), )
"""
encoder is an unbounded memory
"""
decoder_init = BidirectionalGruEncoder(
    name="encoder",
    input_layer="source_embedding",
    hidden_size=latent_chain_dim,
    encoder_depth=1)

MixedLayer(  #for attention
    name="encoder_projected",
    active_type="",
    size=latent_chain_dim,
    bias=False,
    inputs=FullMatrixProjection("encoder"), )
"""
initialize the external memory
"""
ExternalMemoryBankInit(
    name="external_memory_boot",
    encoder_out="encoder",
    perturbation="random_perturbation")

###########################   state to boot memory bank #######################

RecurrentLayerGroupBegin("decoding_layer_group",
                         in_links=[] if generating else ["target_embedding"],
                         out_links=["predicted_word"] if generating else ["decoder_state"],
                         generator=Generator(max_num_frames=max_gen_frame_num,\
                                             beam_size=beam_size,num_results_per_sample=beam_size) if generating else None,
                         )

########################### for generate #################################
if generating:
    predicted_word_memory = Memory(
        name="predicted_word",
        size=trg_word_count,
        boot_with_const_id=start_id, )
    MixedLayer(
        name="target_embedding",
        size=word_vec_dim,
        active_type="",
        bias=False,
        inputs=TableProjection(
            predicted_word_memory,
            initial_std=1 / math.sqrt(word_vec_dim),
            parameter_name="_target_language_embedding"), )
"""
    the following layers are actually data input for the decoder GRU
"""
encoder_out_memory = Memory(
    name="encoder_out",
    size=2 * latent_chain_dim,
    boot_layer="encoder",
    is_sequence=True)
Layer(
    name="encoder_out",
    type="mixed",
    size=2 * latent_chain_dim,
    active_type="",
    bias=False,
    inputs=IdentityProjection(encoder_out_memory), )

encoder_out_proj_memory = Memory(
    name="encoder_out_proj",
    size=latent_chain_dim,
    boot_layer="encoder_projected",
    is_sequence=True)
Layer(
    name="encoder_out_proj",  # read-only memory
    type="mixed",
    size=latent_chain_dim,
    active_type="",
    bias=False,
    inputs=IdentityProjection(encoder_out_proj_memory), )
"""
    decoder state ( t - 1 ) must be defined and INITIALIZED first,
    because it is referred to by read head, candidate state computation and so on
"""
decoder_state_memory = Memory(
    name="decoder_state",
    boot_layer=decoder_init,
    boot_bias=False,
    size=latent_chain_dim,
    is_sequence=False)

######################### external memory #############################
# here, external memory is actuall a sequence with fixed time step.
"""
    here, external memory is actually a sequence ( with a fixed and predefined
    length ) in paddle,
    maybe there are other solutions ? such as using a vector with a length of
    n * latent_chain_dim and use subsequence layer for addressing a certain slot.
    I guess, use a sequence maybe more convient to achieve sequence-wise addressing
"""
external_memory = Memory(
    name="external_memory",  # name of exteral memory here is hard-coded
    size=latent_chain_dim,
    boot_layer="external_memory_boot",
    is_sequence=True)
"""
    all dimensions of the read weights exposed by the read head are
    initialized zero.
"""
ReadExternalMemory(
    read_result_vec="read_res_vec",  # final result reading from external memory
    read_head_weight="read_head_weight",  # read weight exposed by read head at the current time step
    size=latent_chain_dim,
    read_head_boot="zero_read_weight",
    prev_decoder_state=decoder_state_memory,
    external_memory=external_memory)
"""
    context based addressing to read the unbounded memory,
    a.k.a the attention mechanism
"""
Layer(
    name="decoder_state_candidate",
    type="mixed",
    size=latent_chain_dim,
    active_type="tanh",
    bias=False,
    inputs=[
        FullMatrixProjection("read_res_vec"),
        FullMatrixProjection("target_embedding")
    ], )

Layer(
    name="decoder_state_candidate_expand",
    type="expand",
    inputs=["decoder_state_candidate", encoder_out_proj_memory], )

Layer(
    name="attention_vecs",
    type="mixed",
    size=latent_chain_dim,
    active_type="tanh",
    bias=False,
    inputs=[
        IdentityProjection(encoder_out_proj_memory),
        IdentityProjection("decoder_state_candidate_expand")
    ], )

Layer(
    name="attention_weight",
    type="mixed",
    size=1,
    active_type="sequence_softmax",
    bias=False,
    inputs=[FullMatrixProjection("attention_vecs")])

Layer(
    name="context_vectors",
    type="scaling",
    inputs=["attention_weight", encoder_out_memory], )

Layer(
    name="context",
    type="average",
    average_strategy="sum",
    inputs=["context_vectors"], )

recurrent.GatedRecurrentUnit(
    name="decoder_state",
    size=latent_chain_dim,
    active_type="tanh",
    gate_active_type="sigmoid",
    inputs=[
        FullMatrixProjection("context"),
        FullMatrixProjection("target_embedding"),
    ],
    out_memory=decoder_state_memory, )

# here write head writes external memory by using the same weight
# that read head used to read external memory
external_memory_name = WriteExternalMemory(
    size=latent_chain_dim,
    cur_decoder_state="decoder_state",
    prev_memory_state=external_memory,
    read_head_weight="read_head_weight")

if generating:
    Layer(
        name="output",
        type="mixed",
        size=trg_word_count,
        active_type="softmax",
        bias=Bias(parameter_name="_output.wbias", initial_std=0),
        inputs=FullMatrixProjection(
            "decoder_state",
            initial_std=1 / math.sqrt(trg_word_count),
            parameter_name="_output.w"), )
    Layer(
        name="predicted_word",
        type="maxid",
        inputs=["output"], )
    Layer(
        name="eos_check",
        type="eos_id",
        eos_id=eos_id,
        inputs=["predicted_word"], )

if not with_cost:
    Evaluator(
        name="frame_debug_printer",
        type="value_printer",
        inputs=["attention_weight"], )
RecurrentLayerGroupEnd("decoding_layer_group")

if generating:
    Evaluator(
        name="target_printer",
        type="seq_text_printer",
        dict_file=dict_file,
        result_file=gen_trans_file,
        inputs=[
            "sent_id",
            "predicted_word",
        ], )
else:
    Layer(
        name="output",
        type="mixed",
        size=trg_word_count,
        active_type="softmax",
        bias=True,
        inputs=FullMatrixProjection(
            "decoder_state", parameter_name="_output.w"), )

    Layer(
        name="cost",
        type="multi-class-cross-entropy",
        inputs=["output", "target_language_next_word"], )

    Evaluator(
        name="token_error_rate",
        type="classification_error",
        inputs=["output", "target_language_next_word"])
